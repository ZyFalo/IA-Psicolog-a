# üß† Asistente Psicoeducativo IA - Estado del Proyecto

**√öltima actualizaci√≥n**: 16 de noviembre de 2025  
**Versi√≥n**: 1.0 - Funcional  
**Hardware**: MacBook Air M4, 16GB RAM

---

## ‚úÖ COMPONENTES FUNCIONALES

### ü§ñ Modelo de IA
- **Modelo**: Qwen/Qwen2.5-7B-Instruct
- **Framework**: MLX (optimizado para Apple Silicon)
- **Cuantizaci√≥n**: 4-bit (~4GB)
- **Ubicaci√≥n**: `models/qwen2.5-7b-mlx/`
- **Rendimiento**: 
  - Carga: ~1.6s
  - Velocidad: ~20 tokens/seg
  - Memoria: ~5GB RAM

### üñ•Ô∏è Backend API (FastAPI)
**Estado**: ‚úÖ Completamente funcional

**Archivos principales**:
```
backend/app/
‚îú‚îÄ‚îÄ main.py                    # Servidor FastAPI
‚îú‚îÄ‚îÄ config.py                  # Configuraci√≥n
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ model_manager_mlx.py  # Gestor del modelo MLX
‚îÇ   ‚îú‚îÄ‚îÄ session_manager.py    # Sesiones + res√∫menes autom√°ticos
‚îÇ   ‚îî‚îÄ‚îÄ guardrails.py         # Detecci√≥n de crisis
‚îî‚îÄ‚îÄ api/
    ‚îú‚îÄ‚îÄ chat.py               # Endpoints de chat
    ‚îú‚îÄ‚îÄ health.py             # Health checks
    ‚îî‚îÄ‚îÄ voice.py              # ‚ö†Ô∏è Placeholders (no implementado)
```

**Endpoints disponibles**:
- `POST /api/chat/message` - Enviar mensaje
- `GET /api/chat/sessions/{id}/history` - Historial
- `POST /api/chat/sessions` - Nueva sesi√≥n
- `GET /api/health` - Estado del sistema

**Iniciar servidor**:
```bash
cd backend
source ../venv/bin/activate
uvicorn app.main:app --reload
```

### üí¨ Chat en Terminal
**Estado**: ‚úÖ Completamente funcional

**Archivo**: `scripts/chat_terminal.py`

**Caracter√≠sticas**:
- Memoria de 20 mensajes
- Res√∫menes autom√°ticos >40 mensajes
- Detecci√≥n de crisis con guardrails
- Comandos: `salir`, `limpiar`, `guardar`, `cargar`

**Ejecutar**:
```bash
source venv/bin/activate
python scripts/chat_terminal.py
```

### üåê Interfaz Web
**Estado**: ‚úÖ Lista (requiere backend activo)

**Archivo**: `frontend/chat.html`

**Abrir**:
```bash
# Terminal 1: Iniciar backend
cd backend && uvicorn app.main:app --reload

# Terminal 2 o navegador:
open frontend/chat.html
```

### üõ°Ô∏è Sistema de Guardrails
**Estado**: ‚úÖ Funcional

**Archivo**: `backend/app/core/guardrails.py`

**Capacidades**:
- Detecci√≥n de crisis (suicidio, autolesi√≥n)
- Filtrado de contenido inapropiado
- Niveles de riesgo: LOW, MEDIUM, HIGH, CRITICAL
- Respuestas de emergencia autom√°ticas
- ~350 l√≠neas, 20+ tests

### üìù Gesti√≥n de Sesiones
**Estado**: ‚úÖ Funcional con res√∫menes autom√°ticos

**Archivo**: `backend/app/core/session_manager.py`

**Caracter√≠sticas**:
- Ventanas de contexto configurables
- Res√∫menes autom√°ticos al superar 40 mensajes
- Expiraci√≥n de sesiones inactivas
- Sistema de mensajes con timestamps
- ~200 l√≠neas, 15+ tests

---

## üì¶ DEPENDENCIAS INSTALADAS

**Archivo**: `backend/requirements-mlx.txt`

```
mlx>=0.29.0              # Framework Apple Silicon
mlx-lm>=0.28.0           # Modelos de lenguaje
torch>=2.0.0             # PyTorch base
transformers>=4.39.0     # Hugging Face
fastapi>=0.100.0         # API REST
uvicorn[standard]        # Servidor ASGI
pydantic>=2.0.0          # Validaci√≥n
```

**Instaladas exitosamente**:
- ‚úÖ MLX 0.29.4
- ‚úÖ torch 2.9.1
- ‚úÖ transformers 4.57.1
- ‚úÖ fastapi 0.121.2
- ‚úÖ accelerate 1.11.0
- ‚úÖ bitsandbytes 0.42.0
- ‚úÖ scipy 1.16.3

---

## üß™ TESTING

**Ubicaci√≥n**: `tests/`

**Tests disponibles**:
- `test_guardrails.py` - 20+ tests de detecci√≥n de crisis
- `test_session_manager.py` - 15+ tests de sesiones

**Ejecutar**:
```bash
pytest tests/ -v
```

---

## üìö DOCUMENTACI√ìN

```
docs/
‚îú‚îÄ‚îÄ ARCHITECTURE.md      # Arquitectura del sistema
‚îú‚îÄ‚îÄ QUICKSTART.md        # Gu√≠a de inicio r√°pido
‚îî‚îÄ‚îÄ MODELO_MLX_SETUP.md  # Setup y rendimiento de MLX
```

**Dataset**:
- `data/DATASET_FORMAT.md` - Especificaci√≥n completa con ejemplos

---

## üöÄ COMANDOS R√ÅPIDOS

### Probar el modelo
```bash
python -c "
from backend.app.core.model_manager_mlx import ModelManagerMLX
manager = ModelManagerMLX()
manager.load_model()
print(manager.generate_chat([{'role': 'user', 'content': 'Hola'}]))
"
```

### Chat interactivo
```bash
python scripts/chat_terminal.py
```

### Iniciar backend
```bash
cd backend
uvicorn app.main:app --reload --port 8000
```

### Ver info del modelo
```bash
python -c "
from backend.app.core.model_manager_mlx import ModelManagerMLX
m = ModelManagerMLX()
m.load_model()
print(m.get_model_info())
"
```

---

## üìä CARACTER√çSTICAS DEL SISTEMA

### Memoria y Contexto
- **Configuraci√≥n actual**: 20 mensajes en memoria
- **Res√∫menes autom√°ticos**: Se activan >40 mensajes
- **Persistencia**: Guardar/cargar conversaciones en JSON
- **Ventana m√°xima**: ~32,768 tokens (l√≠mite del modelo)

### Guardrails
- **Palabras clave crisis**: ~50 patrones
- **Patrones regex**: Detecci√≥n avanzada
- **Respuestas emergencia**: Autom√°ticas con recursos
- **Filtros contenido**: Diagn√≥sticos, prescripciones

### Rendimiento
- **Carga inicial**: 1.6 segundos
- **Primera respuesta**: 2-4 segundos
- **Respuestas siguientes**: 2-3 segundos
- **Velocidad**: ~20 tokens/seg (~12 palabras/seg)
- **Memoria RAM**: ~5GB durante uso

---

## ‚ö†Ô∏è PENDIENTES / NO IMPLEMENTADOS

### Fine-tuning
- ‚ùå Script de fine-tuning con MLX
- ‚ùå Dataset de entrenamiento (500-2000 ejemplos)
- üìù Formato documentado en `data/DATASET_FORMAT.md`

### Voice Features
- ‚ùå Whisper ASR (speech-to-text)
- ‚ùå Piper TTS (text-to-speech)
- üìù Placeholders en `backend/app/api/voice.py` (88 l√≠neas de esqueleto)

### Mejoras Futuras
- [ ] Integraci√≥n con base de datos
- [ ] Autenticaci√≥n de usuarios
- [ ] Dashboard de m√©tricas
- [ ] Logs estructurados
- [ ] Docker deployment
- [ ] Tests de integraci√≥n E2E

---

## üóÇÔ∏è ARCHIVOS OBSOLETOS

Movidos a `_obsolete/`:
- `model_manager.py` - Versi√≥n PyTorch (reemplazada por MLX)
- `download_model.py` - Usaba bitsandbytes incompatible
- `finetune.py` - LoRA para PyTorch (no MLX)
- `export_to_ollama.sh` - No necesario
- Documentaci√≥n antigua

---

## üìà PR√ìXIMOS PASOS SUGERIDOS

1. **Crear dataset de entrenamiento**
   - Formato: JSONL seg√∫n `data/DATASET_FORMAT.md`
   - Cantidad: 500-2000 ejemplos
   - Distribuci√≥n: 40% check-in, 30% t√©cnicas, 20% crisis, 10% res√∫menes

2. **Adaptar fine-tuning a MLX**
   - Usar `mlx-lm.tune` o `mlx-lm.fuse`
   - LoRA con rank 8-16
   - Cuantizaci√≥n 4-bit

3. **Mejorar guardrails**
   - Reducir falsos positivos
   - M√°s patrones de crisis
   - Niveles de riesgo m√°s granulares

4. **Frontend mejorado**
   - React o Vue.js
   - WebSocket para streaming
   - Historial de sesiones

---

## üìû RECURSOS DE CRISIS

El sistema recomienda estos recursos en crisis:

- **M√©xico**:
  - L√≠nea de la Vida: 800-911-2000
  - SAPTEL: 55-5259-8121

- **Espa√±a**:
  - Tel√©fono de la Esperanza: 717-003-717

- **Internacional**:
  - Crisis Text Line: https://www.crisistextline.org/

---

**Estado**: üü¢ Sistema funcional y listo para uso  
**Hardware**: Optimizado para Apple Silicon (M1/M2/M3/M4)  
**Licencia**: Ver LICENSE (si aplica)
