# üß† Asistente Psicoeducativo IA - Estado del Proyecto

**√öltima actualizaci√≥n**: 16 de noviembre de 2025  
**Versi√≥n**: 1.0 - Funcional  
**Hardware**: MacBook Air M4, 16GB RAM

---

## ‚úÖ COMPONENTES FUNCIONALES

### ü§ñ Modelo de IA
- **Modelo**: Qwen/Qwen2.5-7B-Instruct
- **Framework**: MLX (optimizado para Apple Silicon)
- **Cuantizaci√≥n**: 4-bit (~4GB)
- **Ubicaci√≥n**: `models/qwen2.5-7b-mlx/`
- **Rendimiento**: 
  - Carga: ~1.6s
  - Velocidad: ~20 tokens/seg
  - Memoria: ~5GB RAM

### üñ•Ô∏è Backend API (FastAPI)
**Estado**: ‚úÖ Completamente funcional

**Archivos principales**:
```
backend/app/
‚îú‚îÄ‚îÄ main.py                    # Servidor FastAPI
‚îú‚îÄ‚îÄ config.py                  # Configuraci√≥n
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ model_manager_mlx.py  # Gestor del modelo MLX
‚îÇ   ‚îú‚îÄ‚îÄ session_manager.py    # Sesiones + res√∫menes autom√°ticos
‚îÇ   ‚îî‚îÄ‚îÄ guardrails.py         # Detecci√≥n de crisis
‚îî‚îÄ‚îÄ api/
    ‚îú‚îÄ‚îÄ chat.py               # Endpoints de chat
    ‚îú‚îÄ‚îÄ health.py             # Health checks
    ‚îî‚îÄ‚îÄ voice.py              # ‚ö†Ô∏è Placeholders (no implementado)
```

**Endpoints disponibles**:
- `POST /api/chat/message` - Enviar mensaje
- `GET /api/chat/sessions/{id}/history` - Historial
- `POST /api/chat/sessions` - Nueva sesi√≥n
- `GET /api/health` - Estado del sistema

**Iniciar servidor**:
```bash
cd backend
source ../venv/bin/activate
uvicorn app.main:app --reload
```

### üí¨ Chat en Terminal
**Estado**: ‚úÖ Completamente funcional

**Archivo**: `scripts/chat_terminal.py`

**Caracter√≠sticas**:
- Memoria de 20 mensajes
- Res√∫menes autom√°ticos >40 mensajes
- Detecci√≥n de crisis con guardrails
- Comandos: `salir`, `limpiar`, `guardar`, `cargar`

**Ejecutar**:
```bash
source venv/bin/activate
python scripts/chat_terminal.py
```

### üåê Interfaz Web
**Estado**: ‚úÖ Lista (requiere backend activo)

**Archivo**: `frontend/chat.html`

**Abrir**:
```bash
# Terminal 1: Iniciar backend
cd backend && uvicorn app.main:app --reload

# Terminal 2 o navegador:
open frontend/chat.html
```

### üõ°Ô∏è Sistema de Guardrails
**Estado**: ‚úÖ Funcional

**Archivo**: `backend/app/core/guardrails.py`

**Capacidades**:
- Detecci√≥n de crisis (suicidio, autolesi√≥n)
- Filtrado de contenido inapropiado
- Niveles de riesgo: LOW, MEDIUM, HIGH, CRITICAL
- Respuestas de emergencia autom√°ticas
- ~350 l√≠neas, 20+ tests

### üìù Gesti√≥n de Sesiones
**Estado**: ‚úÖ Funcional con res√∫menes autom√°ticos

**Archivo**: `backend/app/core/session_manager.py`

**Caracter√≠sticas**:
- Ventanas de contexto configurables
- Res√∫menes autom√°ticos al superar 40 mensajes
- Expiraci√≥n de sesiones inactivas
- Sistema de mensajes con timestamps
- ~200 l√≠neas, 15+ tests

---

## üì¶ DEPENDENCIAS

**Archivo**: `requirements.txt` (ra√≠z del proyecto)

**Instalaci√≥n**:
```bash
pip install -r requirements.txt
```

**Dependencias Core** (47 paquetes):
```
# MLX Framework
mlx>=0.29.0              # Framework Apple Silicon
mlx-lm>=0.28.0           # Modelos de lenguaje

# ML/NLP
torch>=2.0.0             # PyTorch base
transformers>=4.39.0     # Hugging Face
huggingface-hub>=0.20.0  # Model hub

# Backend API
fastapi>=0.100.0         # API REST
uvicorn[standard]>=0.23.0 # Servidor ASGI
websockets>=12.0         # WebSocket support

# Validaci√≥n
pydantic>=2.0.0          # Data validation
pydantic-settings>=2.0.0 # Settings management

# Testing
pytest>=7.4.0            # Test framework
pytest-asyncio>=0.21.0   # Async tests
httpx>=0.24.0            # HTTP testing
```

**Versiones Instaladas** (probadas):
- ‚úÖ MLX 0.29.4 + mlx-metal 0.29.4
- ‚úÖ PyTorch 2.9.1
- ‚úÖ Transformers 4.57.1
- ‚úÖ FastAPI 0.121.2
- ‚úÖ Uvicorn 0.38.0
- ‚úÖ Pytest 9.0.1
- ‚úÖ Pydantic 2.12.4

**Dependencias Opcionales** (comentadas en requirements.txt):
```bash
# Fine-tuning: datasets, peft, scipy, accelerate, bitsandbytes
# Voice: openai-whisper, piper-tts
```

---

## üß™ TESTING

**Ubicaci√≥n**: `tests/`

**Tests disponibles**:
- `test_guardrails.py` - 20+ tests de detecci√≥n de crisis
- `test_session_manager.py` - 15+ tests de sesiones

**Ejecutar**:
```bash
pytest tests/ -v
```

---

## üìö DOCUMENTACI√ìN

```
docs/
‚îú‚îÄ‚îÄ ARCHITECTURE.md      # Arquitectura del sistema
‚îú‚îÄ‚îÄ QUICKSTART.md        # Gu√≠a de inicio r√°pido
‚îî‚îÄ‚îÄ MODELO_MLX_SETUP.md  # Setup y rendimiento de MLX
```

**Dataset**:
- `data/DATASET_FORMAT.md` - Especificaci√≥n completa con ejemplos

---

## üöÄ COMANDOS R√ÅPIDOS

### Instalaci√≥n Inicial
```bash
# Clonar y configurar
git clone https://github.com/ZyFalo/IA-Psicolog-a.git
cd IA-Psicolog-a
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Descargar modelo (una sola vez, ~30 min)
python -m mlx_lm.convert --hf-path Qwen/Qwen2.5-7B-Instruct \
    --mlx-path ./models/qwen2.5-7b-mlx -q
```

### Probar el modelo
```bash
python -c "
from backend.app.core.model_manager_mlx import ModelManagerMLX
manager = ModelManagerMLX()
manager.load_model()
print(manager.generate_chat([{'role': 'user', 'content': 'Hola'}]))
"
```

### Chat interactivo
```bash
python scripts/chat_terminal.py
```

### Iniciar backend
```bash
cd backend
uvicorn app.main:app --reload --port 8000
```

### Ver info del modelo
```bash
python -c "
from backend.app.core.model_manager_mlx import ModelManagerMLX
m = ModelManagerMLX()
m.load_model()
print(m.get_model_info())
"
```

---

## üìä CARACTER√çSTICAS DEL SISTEMA

### Memoria y Contexto
- **Configuraci√≥n actual**: 20 mensajes en memoria
- **Res√∫menes autom√°ticos**: Se activan >40 mensajes
- **Persistencia**: Guardar/cargar conversaciones en JSON
- **Ventana m√°xima**: ~32,768 tokens (l√≠mite del modelo)

### Guardrails
- **Palabras clave crisis**: ~50 patrones
- **Patrones regex**: Detecci√≥n avanzada
- **Respuestas emergencia**: Autom√°ticas con recursos
- **Filtros contenido**: Diagn√≥sticos, prescripciones

### Rendimiento
- **Carga inicial**: 1.6 segundos
- **Primera respuesta**: 2-4 segundos
- **Respuestas siguientes**: 2-3 segundos
- **Velocidad**: ~20 tokens/seg (~12 palabras/seg)
- **Memoria RAM**: ~5GB durante uso

---

## ‚ö†Ô∏è PENDIENTES / NO IMPLEMENTADOS

### Fine-tuning
- ‚ùå Script de fine-tuning con MLX
- ‚ùå Dataset de entrenamiento (500-2000 ejemplos)
- üìù Formato documentado en `data/DATASET_FORMAT.md`

### Voice Features
- ‚ùå Whisper ASR (speech-to-text)
- ‚ùå Piper TTS (text-to-speech)
- üìù Placeholders en `backend/app/api/voice.py` (88 l√≠neas de esqueleto)

### Mejoras Futuras
- [ ] Integraci√≥n con base de datos
- [ ] Autenticaci√≥n de usuarios
- [ ] Dashboard de m√©tricas
- [ ] Logs estructurados
- [ ] Docker deployment
- [ ] Tests de integraci√≥n E2E

---

## üóÇÔ∏è ARCHIVOS OBSOLETOS

Movidos a `_obsolete/` (12 archivos, no en Git):
- `model_manager.py` - Versi√≥n PyTorch (reemplazada por MLX)
- `download_model.py` - Usaba bitsandbytes incompatible
- `finetune.py` - LoRA para PyTorch (no MLX)
- `export_to_ollama.sh` - No necesario
- `requirements.txt` (viejo) - Reemplazado por unificado
- `requirements-minimal.txt` - Redundante
- `requirements-mlx.txt` - Redundante
- `.env.example` (viejo) - Actualizado en ra√≠z
- `setup.sh` - Obsoleto
- `COMMANDS.sh` - Referencias obsoletas
- Documentaci√≥n antigua (README_old.md, etc.)

---

## üìà PR√ìXIMOS PASOS SUGERIDOS

1. **Crear dataset de entrenamiento**
   - Formato: JSONL seg√∫n `data/DATASET_FORMAT.md`
   - Cantidad: 500-2000 ejemplos
   - Distribuci√≥n: 40% check-in, 30% t√©cnicas, 20% crisis, 10% res√∫menes

2. **Adaptar fine-tuning a MLX**
   - Usar `mlx-lm.tune` o `mlx-lm.fuse`
   - LoRA con rank 8-16
   - Cuantizaci√≥n 4-bit

3. **Mejorar guardrails**
   - Reducir falsos positivos
   - M√°s patrones de crisis
   - Niveles de riesgo m√°s granulares

4. **Frontend mejorado**
   - React o Vue.js
   - WebSocket para streaming
   - Historial de sesiones

---

## üìû RECURSOS DE CRISIS

El sistema recomienda estos recursos en crisis:

- **M√©xico**:
  - L√≠nea de la Vida: 800-911-2000
  - SAPTEL: 55-5259-8121

- **Espa√±a**:
  - Tel√©fono de la Esperanza: 717-003-717

- **Internacional**:
  - Crisis Text Line: https://www.crisistextline.org/

---

**Estado**: üü¢ Sistema funcional y listo para uso  
**Hardware**: Optimizado para Apple Silicon (M1/M2/M3/M4)  
**Licencia**: Ver LICENSE (si aplica)
